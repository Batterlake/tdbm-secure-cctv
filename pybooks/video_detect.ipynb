{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "import face_detect as FD\n",
    "import insightface_detect as ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image\n",
    "\n",
    "face_cascade = FD.initialize()\n",
    "video = cv2.VideoCapture('http://192.168.2.36:4747/video')\n",
    "video.set(3,480) # adjust width\n",
    "video.set(4,480) # adjust height\n",
    "video.set(cv2.CAP_PROP_BUFFERSIZE, 0)\n",
    "display_handle=display(None, display_id=True)\n",
    "try:\n",
    "    while True:\n",
    "        _, frame = video.read()\n",
    "        result = FD.face_detect(frame, face_cascade)\n",
    "        _, result = cv2.imencode('.jpeg', result)\n",
    "        display_handle.update(Image(data=result.tobytes()))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    video.release()\n",
    "    display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insightface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image\n",
    "\n",
    "face_cascade = ID.initialize()\n",
    "video = cv2.VideoCapture('http://192.168.2.36:4747/video')\n",
    "video.set(3,480) # adjust width\n",
    "video.set(4,480) # adjust height\n",
    "video.set(cv2.CAP_PROP_BUFFERSIZE, 10)\n",
    "display_handle=display(None, display_id=True)\n",
    "try:\n",
    "    while True:\n",
    "        _, frame = video.read()\n",
    "        result = ID.person_detect(frame, face_cascade)\n",
    "        _, result = cv2.imencode('.jpeg', result)\n",
    "        display_handle.update(Image(data=result.tobytes()))\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    video.release()\n",
    "    display_handle.update(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "from deepface.detectors import FaceDetector\n",
    "from deepface.commons import functions\n",
    "\n",
    "\n",
    "def detect_face(img, backend, align = True):\n",
    "\n",
    "    #detector stored in a global variable in FaceDetector object.\n",
    "    #this call should be completed very fast because it will return found in memory\n",
    "    #it will not build face detector model in each call (consider for loops)\n",
    "    face_detector = FaceDetector.build_model(backend)\n",
    "    detected_face = None\n",
    "    img_region = None\n",
    "    try:\n",
    "        detected_face, img_region = FaceDetector.detect_face(face_detector, backend, img, align)\n",
    "    except:\n",
    "        pass\n",
    "    return detected_face, img_region\n",
    "\n",
    "# def detect_face(detector, wrapper, img, align = True):\n",
    "\n",
    "#     faces = wrapper(detector, img, align)\n",
    "#     return faces\n",
    "\n",
    "# def resize_image(img, target_size=(224,224), grayscale=False):\n",
    "#     factor_0 = target_size[0] / img.shape[0]\n",
    "#     factor_1 = target_size[1] / img.shape[1]\n",
    "#     factor = min(factor_0, factor_1)\n",
    "\n",
    "#     dsize = (int(img.shape[1] * factor), int(img.shape[0] * factor))\n",
    "#     img = cv2.resize(img, dsize)\n",
    "\n",
    "#     # Then pad the other side to the target size by adding black pixels\n",
    "#     diff_0 = target_size[0] - img.shape[0]\n",
    "#     diff_1 = target_size[1] - img.shape[1]\n",
    "#     if grayscale == False:\n",
    "#         # Put the base image in the middle of the padded image\n",
    "#         img = np.pad(img, ((diff_0 // 2, diff_0 - diff_0 // 2), (diff_1 // 2, diff_1 - diff_1 // 2), (0, 0)), 'constant')\n",
    "#     else:\n",
    "#         img = np.pad(img, ((diff_0 // 2, diff_0 - diff_0 // 2), (diff_1 // 2, diff_1 - diff_1 // 2)), 'constant')\n",
    "        \n",
    "#     if img.shape[0:2] != target_size:\n",
    "#         img = cv2.resize(img, target_size)\n",
    "        \n",
    "#     return img\n",
    "\n",
    "# def normalize(image):\n",
    "#     img_pixels = np.expand_dims(image, axis = 0)\n",
    "#     img_pixels /= 255 #normalize input in [0, 1]\n",
    "#     return img_pixels\n",
    "\n",
    "# def preprocess_face(face_img, target_size=(224, 224)):\n",
    "#     image = resize_image(face_img, target_size)\n",
    "#     norm_image = normalize(image)\n",
    "#     return norm_image\n",
    "\n",
    "\n",
    "\n",
    "recognizer = DeepFace.build_model('ArcFace')\n",
    "detector_backends = [\n",
    "    'opencv', \n",
    "    'ssd', \n",
    "    'dlib', \n",
    "    'mtcnn', \n",
    "    'retinaface', \n",
    "    'mediapipe'\n",
    "]\n",
    "be = detector_backends[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.3 ms, sys: 342 µs, total: 2.65 ms\n",
      "Wall time: 2.29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for _ in range(10):\n",
    "    recognizer = DeepFace.build_model('ArcFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844 ns ± 4.12 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "recognizer = DeepFace.build_model('ArcFace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_face_image(img, target_size = (224,224,), grayscale=False):\n",
    "    factor_0 = target_size[0] / img.shape[0]\n",
    "    factor_1 = target_size[1] / img.shape[1]\n",
    "    factor = min(factor_0, factor_1)\n",
    "\n",
    "    dsize = (int(img.shape[1] * factor), int(img.shape[0] * factor))\n",
    "    img = cv2.resize(img, dsize)\n",
    "\n",
    "    # Then pad the other side to the target size by adding black pixels\n",
    "    diff_0 = target_size[0] - img.shape[0]\n",
    "    diff_1 = target_size[1] - img.shape[1]\n",
    "    if grayscale == False:\n",
    "        # Put the base image in the middle of the padded image\n",
    "        img = np.pad(img, ((diff_0 // 2, diff_0 - diff_0 // 2), (diff_1 // 2, diff_1 - diff_1 // 2), (0, 0)), 'constant')\n",
    "    else:\n",
    "        img = np.pad(img, ((diff_0 // 2, diff_0 - diff_0 // 2), (diff_1 // 2, diff_1 - diff_1 // 2)), 'constant')\n",
    "\n",
    "    if img.shape[0:2] != target_size:\n",
    "        img = cv2.resize(img, target_size)\n",
    "\n",
    "    img_pixels = np.expand_dims(img, axis = 0).astype(np.float32)\n",
    "    img_pixels /= 255 #normalize input in [0, 1]\n",
    "    \n",
    "    return img_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import display, Image, JSON\n",
    "\n",
    "video = cv2.VideoCapture('http://192.168.2.36:4747/video')\n",
    "video.set(3,640) # adjust width\n",
    "video.set(4,640) # adjust height\n",
    "video.set(cv2.CAP_PROP_BUFFERSIZE, 0)\n",
    "display_handle=display(None, display_id=True)\n",
    "try:\n",
    "    while True:\n",
    "        _, frame = video.read()\n",
    "        face, location = detect_face(frame, be, align=False)\n",
    "        \n",
    "        embedding = None\n",
    "        result = frame.copy()\n",
    "        if face is not None and len(face) != 0:\n",
    "            result = face.copy()\n",
    "            img = preprocess_face_image(face, target_size=(112,112))\n",
    "            img = functions.normalize_input(img = img, normalization = 'base')\n",
    "            embedding = recognizer(img).numpy()[0].tolist()\n",
    "        \n",
    "        _, result = cv2.imencode('.jpeg', result)\n",
    "        \n",
    "#         display_handle.update(JSON(data=embedding))\n",
    "        display_handle.update(Image(data=result.tobytes()))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    video.release()\n",
    "    display_handle.update(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess_face_image(face, target_size=(112,112))\n",
    "img = functions.normalize_input(img = img, normalization = 'base')\n",
    "embedding = recognizer(img).numpy()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface.commons import functions\n",
    "\n",
    "\n",
    "input_shape_x, input_shape_y = functions.find_input_shape(model)\n",
    "\n",
    "\t#detect and align\n",
    "\timg = functions.preprocess_face(img = img_path\n",
    "\t\t, target_size=(input_shape_y, input_shape_x)\n",
    "\t\t, enforce_detection = enforce_detection\n",
    "\t\t, detector_backend = detector_backend\n",
    "\t\t, align = align)\n",
    "\n",
    "\t#---------------------------------\n",
    "\t#custom normalization\n",
    "\n",
    "\timg = functions.normalize_input(img = img, normalization = normalization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "a520e75aa5f4261f13eb8f56292a78b8fd013d53d463f35c9f2b77664eb75a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
